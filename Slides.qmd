---
title: "Working with Geospatial Hydrologic Data for Watershed Analyses in R and Python Using Web Services"
format: 
  html:
    toc: true
    toc-depth: 4
    toc-location: left
    self-contained: true
    code-link: true
    code-fold: true
editor: visual
date: 2023-06-05
---

## Introduction

- Housekeeping
- Introductions
- Agenda:
  1. Key Building blocks in R and Python 
  2. Examples of fundamental Tools in R
  3. Examples of fundamental Tools in Python
  4. Use Cases and Worked Examples
  5. Demonstration of Watershed Analysis with `Leafmap` and `WhiteboxTools` (leafmap.org/workshops/ICRW_2023/)
  
- Framework:

This document and all of the workshop material are available at this GitHub repository:

- https://github.com/mhweber/ICRW8_Geospatial_Workshop

**To Follow Along with the Code**

- This document is built in RStudio with `Quarto` and rendered to html - all code sections can be run in RStudio in the code blocks in `Quarto`, or copied and pasted into a new R script document
- If you are familiar with using git and GitHub, you can fork and clone the repository and run locally - if not, you can simply copy and paste code to follow along with examples
- Python code examples are in jupyter notebooks 
    - if you have successfully set up an environment on your local machine you can run notebooks there
    - alternatively, you can use the `binder` link we have set up to run jupyter notebooks in a binder environment


*The views expressed in this presentation are those of the authors and do not necessarily represent the views or policies of the U.S. Environmental Protection Agency or the U.S.G.S.*

*This land is ancestral and unceded land of the Chepenafa (Mary’s River) Band of the [Kalapuya](https://libraryguides.lanecc.edu/kalapuya) (pronounce “cal-uh-poo-yuh”), who have stewarded this beautiful area for generations.  The descendants of the Kalapuya People are now part of the federally recognized [Confederated Tribes of Grand Ronde](https://www.grandronde.org/) and [Confederated Tribes of the Siletz](https://www.ctsi.nsn.us/) of Oregon. The peak you see from numerous vantage points around Corvallis is Mary's Peak, which the Kalapuya called tcha Timanwi, or 'place of spiritual power', and is the highest peak in the Oregon Coast Range (4,097 feet).*

*The views expressed in this presentation are those of the authors and do not necessarily represent the views or policies of the U.S. Environmental Protection Agency or the U.S.G.S.*

*This land is ancestral and unceded land of the Chepenafa (Mary’s River) Band of the [Kalapuya](https://libraryguides.lanecc.edu/kalapuya) (pronounce “cal-uh-poo-yuh”), who have stewarded this beautiful area for generations.  The descendants of the Kalapuya People are now part of the federally recognized [Confederated Tribes of Grand Ronde](https://www.grandronde.org/) and [Confederated Tribes of the Siletz](https://www.ctsi.nsn.us/) of Oregon. The peak you see from numerous vantage points around Corvallis is Mary's Peak, which the Kalapuya called tcha Timanwi, or 'place of spiritual power', and is the highest peak in the Oregon Coast Range (4,097 feet).*

## Key Concepts Across Both languages

### Core Libraries

-   Spatial data structures across languages and applications is primarily organized through [OSgeo](https://www.osgeo.org) and [OGC](https://www.ogc.org)) *and* a few core libraries underpin spatial libraries in programming languages and in software applications(R, Python, QGIS, ArcPro)!

These libraries include:

-   **PROJ** --\> Spatial projections, transformations

-   **GEOS** --\> Geometry operations (measures, relations)

-   **GDAL** --\> Raster and feature abstraction and processing (read, write)

-   **NetCDF** --\> Multidimensional (XYZT) data abstraction (read, write)

### Requesting web services (APIs)

More and more data is available as web services, and the data is stored in a remote server in JSON, XML, HTML format which is accessible through API's.

**JSON** (JavaScript Object Notation) is a lightweight data-interchange format which is easy for machines to generate and parse; easy for humans to write and read.

Sample JSON format : { "data": "Click Here", "size": 36, "style": "bold", "name": "text1", }

**API** - An Application Programming Interface (API) takes a structured request from an application and returns structured results from the host application. With a **Rest API** you're getting a representation of the requested data stored in a server. A **Rest API** also is what we call 'stateless', which means a server doesn't store any data between requests from clients.

**Rest APIs** access data through **uniform resource identifiers (URIs)**, which are essentially a string of characters that identify a specific resource. The type of URI used by a **Rest API** is a **uniform resource locator (URL)**.

**HTTP** clients are used for accessing the API. **HyperText Transfer Protocol (HTTP)** enables communication between the client and server using **HTTP** methods. If we want to access or manipulate resources a **Rest API** uses specific request verbs we need to become familiar with:

-   GET: used to acquire data from a database

-   POST: used to add data to a database

-   PUT: update the data in a database

-   DELETE: delete data in a database

Although web services facilitate accessing various datasets, we have to be wary
of their limitations. Creating and maintaining a web service is a technically
challenging and involved task, and it becomes even more complicated when the
service becomes public. For public web services, number of users and load of
their request is not predictable. Thus, public web services often have
limitations and quotas on the number of requests. Considering that there
are workarounds for these limitations, some users, wittingly or unwittingly,
can cause a web service to slowdown or even shut down. So, we should expect
planned or unplanned downtimes when using web services. Planned downtimes
are usually announced by the service provider that can be for maintenance
purposes or making some changes to the service. Keeping up with these changes
is also another aspect that we need to take into account when working with web service.

Additionally, sending a request and retrieving a response from a web service
(a web service call) has an inherent overhead and is much slower than accessing
a local file. Thus, web services might not be suitable for "large" requests,
and it might be more efficient to download the dataset and store it locally.
How "large" is large, depends on the capacity of a web service and the
complexity of the request. For example, the NLDI web service has an endpoint
that can navigate the NHD network up or downstream of a point of interest up
to a given distance. This call is computationally expensive thus making many
such requests can impact the performance of the web service. For such cases,
it is more efficient to download the NHDPlus dataset and perform the navigation
locally. For example, [NHDPlusTools](https://github.com/doi-usgs/nhdplustools/)
in R and [PyNHD](https://docs.hyriver.io/readme/pynhd.html) in Python provide tools to navigate the NHDPlus dataset locally.

By in large, web data access will fall into three broad categories. 
1. APIs, 
1. Object Stores, and 
1. File Servers. 

#### APIs

When working with an API, you are interacting with a remote computer that may 
or may not be prepared to cope with many concurrent requests. The computer may 
have less compute power than your laptop and it may be connected to an 
inexpensive storage device that can only provide a few MB/s of throughput. 
When working with APIs, you should always:

1. start small and check latency per request.
1. scale up request size and check response time scaling.
1. increase parallelism slowly and check response performance.

The code below demonstrates this using requests to a geospatial web service.


```{r}

point <- sf::st_sfc(sf::st_point(c(-76.61612, 39.23875)), crs = 4326)

# just baseline request time
start_comid <- nhdplusTools::discover_nhdplus_id(point)

upstream_comid <- dataRetrieval::findNLDI(start_comid, nav = "UT", distance_km = 999)

comids <- as.integer(upstream_comid$UT_flowlines$nhdplus_comid)

sf::sf_use_s2(FALSE) # hides warnings

# first, just check per request timing.
# if we had a lot to run, simple match will tell us how long to expect.
system.time(nhdplusTools::subset_nhdplus(comids[2], 
                                         nhdplus_data = "download", 
                                         status = FALSE))

# Now try scaling up.
system.time(nhdplusTools::subset_nhdplus(comids[1:100], 
                                         nhdplus_data = "download", 
                                         status = FALSE))

# scale up further and observe timing
system.time(nhdplusTools::subset_nhdplus(comids[1:400], 
                                         nhdplus_data = "download", 
                                         status = FALSE))

# we can experiment with parallelism
future::plan(future::multisession, workers = 2)

future::future(sf::sf_use_s2(FALSE))

# run in two chunks and two workers
system.time(future.apply::future_lapply(list(comids[1:200], comids[201:400]), function(x) {
  suppressMessages(nhdplusTools::subset_nhdplus(x, 
                               nhdplus_data = "download", 
                               status = FALSE))
}, future.seed = TRUE))

# run individually with 2 workers
system.time(future.apply::future_lapply(comids[1:200], function(x) {
  suppressMessages(nhdplusTools::subset_nhdplus(x, 
                               nhdplus_data = "download", 
                               status = FALSE))
}, future.seed = TRUE))

future::plan(future::multisession, workers = 8)

# run individually with 8 workers
# notice that this is faster but not 4x taster
system.time(future.apply::future_lapply(comids[1:200], function(x) {
  suppressMessages(nhdplusTools::subset_nhdplus(x, 
                               nhdplus_data = "download", 
                               status = FALSE))
}, future.seed = TRUE))
```

#### Object Stores and File Servers

Object stores are different than APIs in that they are designed to handle parallelism. If downloading many files from an object store, doing so in parallel can increase total through put assuming you have overall bandwidth to spare. A file server, if not backed by an object store will typically not support parallelism as files are not necessarily stored on a file system that is designed with parallel access in mind. It's always a good idea to do some testing at small scale if you have long running processes that work over the internet to make sure you are using a pattern that makes sense with the baseline performance characteristics of the server.

### Geospatial data representation fundamentals: simple features and geospatial grids

-   \***Vector** data are comprised of points, lines, and polygons that represent discrete spatial entities, such as a river, watershed, or stream gauge.

-   **Raster** data divides spaces into rectilinear cells (pixels) to represent spatially continuous phenomena, such as elevation or the weather. The cell size (or resolution) defines the fidelity of the data.

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/09-vec-raster.jpg")
```

For *Vector* data Simple Features (officially Simple Feature Access) is both an OGC and International Organization for Standardization (ISO) standard that specifies how (mostly) two-dimensional geometries can represent and describe objects in the real world. Simple features includes:

-   a class hierarchy
-   a set of operations
-   binary and text encodings

It describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.

It outlines how the spatial elements of POINTS (XY locations with a specific coordinate reference system) extend to LINES, POLYGONS and GEOMETRYCOLLECTION(s).

The "simple" adjective also refers to the fact that the line or polygon geometries are represented by sequences of points connected with straight lines that do not self-intersect.

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/09-sf-model.png")
```

#### Simple and valid geometries and ring direction

This breakdown of simple features follows for the most part [this section in Spatial Data Science](https://r-spatial.org/book/03-Geometries.html) For *linestrings* to be considered *simple* they must not self-intersect:

```{r}
library(sf)
(ls <- st_linestring(rbind(c(0,0), c(1,1), c(2,2), c(0,2), c(1,1), c(2,0))))
```

```{r}
#| message: false
#| echo: false
#| error: false
plot(ls)
```

```{r}
#| message: false
#| echo: false
#| error: false
c(is_simple = st_is_simple(ls))
```

For *polygons* several other conditions have to be met to be *simple*:

-   polygon rings are closed (the last point equals the first)
-   polygon holes (inner rings) are inside their exterior ring
-   polygon inner rings maximally touch the exterior ring in single points, not over a line
-   a polygon ring does not repeat its own path
-   in a multi-polygon, an external ring maximally touches another exterior ring in single points, not over a line

*z* and *m* coordinates As well as having the necessary X and Y coordinates, single point (vertex) simple features can have:

-   a Z coordinate, denoting altitude, and/or
-   an M value, denoting some "measure"

**Text and binary encodings** A key part of the standard feature encoding is **text** and **binary** encodings. The well-known text (WKT) encoding we have shown above gives us a human-readable description of the geometry. The well-known binary (WKB) encoding is machine-readable, lossless, and faster to work with than text encoding. WKB is used for all interactions with `GDAL` and `GEOS`.

**Operations on geometries** We can break down operations on geometries for *vector* features in the following way:

-   **predicates**: a logical asserting a certain property is `TRUE`
-   **measures**: a quantity (a numeric value, possibly with measurement unit)
-   **transformations**: newly generated geometries

We can look at these operations by **what** they operate on, whether the are single geometries, pairs, or sets of geometries:

-   **unary** when it's a single geometry
-   **binary** when it's pairs of geometries
-   **n-ary** when it's sets of geometries

**Unary** predicates work to describe a property of a geometry.

A list of unary predicates:

| predicate   | meaning                                         |
|-------------|-------------------------------------------------|
| `is`        | Tests if geometry belongs to a particular class |
| `is_simple` | Tests whether geometry is simple                |
| `is_valid`  | Test whether geometry is valid                  |
| `is_empty`  | Tests if geometry is empty                      |

A list of binary predicates is:

| predicate            | meaning                                                                                                             | inverse of   |
|------------------|------------------------------------|------------------|
| `contains`           | None of the points of A are outside B                                                                               | `within`     |
| `contains_properly`  | A contains B and B has no points in common with the boundary of A                                                   |              |
| `covers`             | No points of B lie in the exterior of A                                                                             | `covered_by` |
| `covered_by`         | Inverse of `covers`                                                                                                 |              |
| `crosses`            | A and B have some but not all interior points in common                                                             |              |
| `disjoint`           | A and B have no points in common                                                                                    | `intersects` |
| `equals`             | A and B are topologically equal: node order or number of nodes may differ; identical to A contains B and A within B |              |
| `equals_exact`       | A and B are geometrically equal, and have identical node order                                                      |              |
| `intersects`         | A and B are not disjoint                                                                                            | `disjoint`   |
| `is_within_distance` | A is closer to B than a given distance                                                                              |              |
| `within`             | None of the points of B are outside A                                                                               | `contains`   |
| `touches`            | A and B have at least one boundary point in common, but no interior points                                          |              |
| `overlaps`           | A and B have some points in common; the dimension of these is identical to that of A and B                          |              |
| `relate`             | Given a mask pattern, return whether A and B adhere to this pattern                                                 |              |

See the [Geometries chapter of Spatial Data Science](https://r-spatial.org/book/03-Geometries.html) for a full treatment that also covers \*\*unary and binary measures\* as well as *unary, binary and n-ary transformers*

#### Raster data model

You can read the [GDAL raster data model](https://gdal.org/user/raster_data_model.html) and [OpenGIS Grid Coverages specification](https://docs.geotools.org/stable/javadocs/org/opengis/coverage/grid/package-summary.html) but with a raster data model (a cell-based tesselation) you can read / write and operate on raster data in numerous formats using **gdal**:

```{r}
#| message: false
#| echo: false
#| error: false
library(terra)

DT::datatable(gdal(drivers = TRUE))
```

### Drainage basins and catchments - mainstems and flowpaths

When writing software for hydrologic data, concrete conceptual and logical definitions for the spatial features our software works with are critical to enabling interoperability. Along these lines, some key definitions to consider:

-   **Catchment**: A physiographic unit with zero or one inlets and one outlet. A catchment is represented by one or more partial realizations; flowpath, divide, and networks of flowpaths and divides.
-   **Nexus**: Conceptual outlet for water contained by a catchment. The hydro nexus concept represents the place where a catchment interacts with another catchment.
-   **Flowpath**: A flowpath is a linear geometry that represents the connection between a catchment's inlet and its outlet. All flowpaths have a local drainage area and may be aggregates of flowlines.
-   **Flowline**: A flowline is a linear geometry that represents a segment of a flowing body of water. Some flowlines have no local drainage area and are never aggregate features.
-   **Drainage Basin**: A catchment with zero inlets and one (internal or external) outlet.
-   **Mainstem**: The flowpath of a drainage basin.

In the following, we create a representation of catchment divides and flowpaths with the hydrologic locations that represent their nexuses.

**Note:** both the divide and flowpath are said to be realizations of the overall catchment that they represent. In other words, the divide and flowpath are both part of the representation of a catchment.

```{r}
source(system.file("extdata/new_hope_data.R", package = "nhdplusTools"))

catchment <- sf::st_geometry(new_hope_catchment)
flowpath <- sf::st_geometry(dplyr::filter(new_hope_flowline, COMID %in% new_hope_catchment$FEATUREID))
nexus <- sf::st_geometry(nhdplusTools::get_node(flowpath, "end"))


plot_window <- sf::st_as_sfc(sf::st_bbox(dplyr::filter(new_hope_catchment, FEATUREID %in% c(8891178, 8895564))))

par(mar = c(0,0,0,0))
plot(plot_window, col = NA, border = NA)
plot(catchment, col = NA, border = "grey25", add = TRUE)
plot(flowpath, col = "dodgerblue3", add = TRUE)
plot(nexus, col = "springgreen", add = TRUE, pch = 20)

```

In this example, we create a drainage basin boundary (divide), mainstem flowpath of the drainage basin, and the flowlines that make up the hydrographic network of the drainage basin.

**Fun Fact:** the word "watershed" has come to refer to the land encompassed by a drainage divide but this usage is not correct. A watershed is "a dividing ridge between drainage areas" which is in line with the non hydrologic use of the word: a crucial dividing point, line, or factor.

```{r}
basin <- sf::st_geometry(sf::st_union(new_hope_catchment, by_feature = FALSE))
flowline <- sf::st_geometry(new_hope_flowline)
mainstem <- sf::st_geometry(new_hope_flowline[new_hope_flowline$LevelPathI == min(new_hope_flowline$LevelPathI),])

par(mar = c(0,0,0,0))
plot(basin, lwd = 2) 
plot(flowline, col = "dodgerblue", add = TRUE)
plot(mainstem, col = "blue", lwd = 2, add = TRUE)
```

## Demonstration of Key Concepts in Each Language

-   drainage basins and catchments - mainstems and flowpaths
-   geospatial data representation fundamentals, simple features and geospatial grids

### Examples in R

For more detiled treatment see [material for US EPA R User Group 2021 workshop](https://mhweber.github.io/R-User-Group-Spatial-Workshop-2021/)

#### sf: simple features

In R, the [`sf` package](https://cran.r-project.org/web/packages/sf/index.html) provides "*support for simple features, a standardized way to encode spatial vector data.... \[and\] Binds to 'GDAL' for reading and writing data, to 'GEOS' for geometrical operations, and to 'PROJ' for projection conversions and datum transformations.*"

When using R, you' are're using an interface to the core community standards, software, and practices (this isn't exclusive to R). TO highlight this we can install (do this once) and attach `sf` to view the external dependencies versions of the libraries linked to `sf`.

```{r}
#| message: false
# install.packages("sf")
library(sf)

sf_extSoftVersion()
```

The bindings to these lower-level C libraries, and, the larger `sf` ecosystem in R can be seen below:

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/09-sf-depends.png")
```

In the `sf` implementation in the R ecosystem stores simple feature geometries (sfg) as part of a larger data.frame using a simple feature geometry list-column (sfg). The collection of attribute and spatial information define a simple feature that can be operated on in both table (SQL) and spatial (GEOS, etc) contexts. Not only does this allow us to make the most use of the growing spatial community but *also* of the growing data science community (see `ggplot`, `dplyr`, `data.table`, `dbplyr`, `arrow`, etc.)

In practice, an `sf` object in R looks like the following:

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/sf_xfig.png")
```

We can break this down following examples presented in the recently published [Spatial Data Science](https://r-spatial.org/book/03-Geometries.html) by Edzar Pebesma and Rober Bivand. The most common simple feature geometries used to represent a *single* feature are:

| type                 | description                                                        |
|-----------------------|-------------------------------------------------|
| `POINT`              | single point geometry                                              |
| `MULTIPOINT`         | set of points                                                      |
| `LINESTRING`         | single linestring (two or more points connected by straight lines) |
| `MULTILINESTRING`    | set of linestrings                                                 |
| `POLYGON`            | exterior ring with zero or more inner rings, denoting holes        |
| `MULTIPOLYGON`       | set of polygons                                                    |
| `GEOMETRYCOLLECTION` | set of the geometries above                                        |

```{r}
library(sf) |> suppressPackageStartupMessages()
par(mfrow = c(2,4))
par(mar = c(1,1,1.2,1))

# 1
p <- st_point(0:1)
plot(p, pch = 16)
title("point")
box(col = 'grey')

# 2
mp <- st_multipoint(rbind(c(1,1), c(2, 2), c(4, 1), c(2, 3), c(1,4)))
plot(mp, pch = 16)
title("multipoint")
box(col = 'grey')

# 3
ls <- st_linestring(rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)))
plot(ls, lwd = 2)
title("linestring")
box(col = 'grey')

# 4
mls <- st_multilinestring(list(
  rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)),
  rbind(c(3,0), c(4,1), c(2,1))))
plot(mls, lwd = 2)
title("multilinestring")
box(col = 'grey')

# 5 polygon
po <- st_polygon(list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),
    rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))))
plot(po, border = 'black', col = '#ff8888', lwd = 2)
title("polygon")
box(col = 'grey')

# 6 multipolygon
mpo <- st_multipolygon(list(
    list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),
        rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))),
    list(rbind(c(3,7), c(4,7), c(5,8), c(3,9), c(2,8), c(3,7)))))
plot(mpo, border = 'black', col = '#ff8888', lwd = 2)
title("multipolygon")
box(col = 'grey')

# 7 geometrycollection
gc <- st_geometrycollection(list(po, ls + c(0,5), st_point(c(2,5)), st_point(c(5,4))))
plot(gc, border = 'black', col = '#ff6666', pch = 16, lwd = 2)
title("geometrycollection")
box(col = 'grey')
```

```{r}
p
mp
ls
mls
po
mpo
gc
```

This extends the idea of ["tidy" data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) in that each row represents one observation, which has one geometric representation of the real world feature it describes.

An example of basic use:

```{r}
# Define file path
filename <- system.file("shape/nc.shp", package="sf")

# read in file
(nc <- read_sf(filename))
```

```{r}
# Map
plot(nc['SID79'])

# Spatial measures!
head(st_area(nc))

# Spatial operations!
{
  st_union(nc) |> plot()
  
  st_centroid(nc)$geometry |> plot(col = "red", add = TRUE)
}

# data science operations
library(dplyr)

{
 plot(nc$geometry)
 plot(slice_max(nc, AREA, n = 10)$geometry, 
      col = "red", add = TRUE)
 plot(slice_max(nc, AREA, n =5)$geometry, 
      col = "yellow", add = TRUE)
 plot(slice_max(nc, AREA, n = 1)$geometry, 
      col = "green", add = TRUE)
}

```

#### Measure (GEOS Measures)

Measures are the questions we ask about the dimension of a geometry, once a coordinate reference system has been established and we can compute: - How long is a line or polygon perimeter? (unit) - What is the area of a polygon? \*(unit\^2) - How far are two objects from one another? (unit)

-   *Measures* come from the GEOS library
-   *Measures* are calculated in the **units** of the projection

We'll demonstrate with a toy dataset of USGS gages in a data package from a previous workshop:

```{r}
# remotes::install_github("mhweber/awra2020spatial")
gages <- read.csv(system.file("extdata", "Gages_flowdata.csv", package = "awra2020spatial")) |> 
  dplyr::select(SOURCE_FEA, STATE, LAT_SITE, LON_SITE)
```

```{r, echo = FALSE}
dplyr::glimpse(gages)
```

Using `st_as_sf` in `sf` we can easily make a data frame with coordinates into a simple features data frame

```{r}
gages <- gages |> 
  sf::st_as_sf(coords = c("LON_SITE", "LAT_SITE"), crs = 4269)
```

```{r,  echo = FALSE}
dplyr::glimpse(gages)
```

What is the distance from the first stream gage to the second stream gage?

```{r}
sf::st_distance(gages[1,], gages[2,])
```

We can generate toy area by buffering a point (to make a polygon) and find area and perimeter. We need to project into a planar CRS to get perimeter so we will project to an Albers projection using an `epsg` code:

```{r}
poly <- sf::st_buffer(gages[1,],200)
sf::st_area(poly)
poly <- sf::st_transform(poly, 5070)
lwgeom::st_perimeter(poly)
```

#### terra for raster data

We'll look at using [terra](https://rspatial.github.io/terra/reference/terra-package.html) for working with raster data in R - [stars](https://r-spatial.github.io/stars/) is another library for working with raster data and spatiotemporal arrays (raster and vector data cubes) but for the sake of time we won't demonstrate stars in this workshop.

`terra` builds on the older `raster` package and provides methods for low-level data manipulation as well as high-level global, local, zonal, and focal computations. The predict and interpolate methods facilitate the use of regression type (interpolation, machine learning) models for spatial prediction, including with satellite remote sensing data. Processing of very large files is supported.

Like `sf`, `terra` links to GDAL, and the version and drivers can be viewed using package support functions:

```{r}
# install.packages(terra)
library(terra)

gdal()
```

```{r}
DT::datatable(gdal(drivers = TRUE))
```

We can get raster data the [National Map family of APIs](https://apps.nationalmap.gov/services/) and in particular 3DEP Elevation data and work with it using `terra` or `stars`. We'll use Mike Johnson's `AOI` package to get a county polygon to use as a template to pass to `terrainr` for our area of interest

```{r}
library(terrainr)
# remotes::install_github("mikejohnson51/AOI")
library(AOI)
library(mapview)
mapviewOptions(fgb=FALSE)

AOI::aoi_get(list("Corvallis, OR", 10, 10)) |> mapview()
```

```{r}
# aoi <- AOI::aoi_get(state = "OR", county = "Benton")
aoi <- aoi_get(list("Corvallis, OR", 10, 10))
output_tiles <- get_tiles(aoi,
                          services = c("elevation", "ortho"),
                          resolution = 30 # pixel side length in meters
                          )
```

```{r}

terra::plot(terra::rast(output_tiles[["elevation"]][[1]]))
```

```{r}
terra::plotRGB(terra::rast(output_tiles[["ortho"]][[1]]),scale = 1)
```

We can also use the \[elevatr\] package by Jeff Hollister for accessing elevation data through web services and clip out elevation to a watershed basin we access through `nhdplusTools` and the `NLDI`.

Note that `get_elev_raster` from `elevatr` will generate a set of warnings, particularly regarding retirement of `rgdal` - the package will need to be updated to conform with [retirement of rgdal, rgeos and maptools](https://r-spatial.org/r/2022/04/12/evolution.html).

```{r}
#| message: false
#| error: false
library(nhdplusTools)
library(elevatr)
library(mapview)
mapviewOptions(fgb=FALSE)
# We're passing an identifier for the stream reach (and catchment) that we know is the downstream-most segment on the Calapooia River using the COMID below
start_comid = 23763529
nldi_feature <- list(featureSource = "comid", featureID = start_comid)
basin <- nhdplusTools::get_nldi_basin(nldi_feature = nldi_feature)

x <- get_elev_raster(basin, z = 12)
# x is returned as a raster object rather than a terra spatraster, so we need to convert to spatraster
mapview::mapview(basin) + mapview(x)
```

We can also crop the elevation raster to our basin using `terra` - note that `mapview` expects `raster` objects rather than `terra` `SpatRasters` so we need to convert between `terr` and `raster`.

```{r}
library(raster)
x <- terra::mask(terra::rast(x), basin)
x <- raster::raster(x)
mapview::mapview(basin, alpha.regions = 0.02, color='blue', lwd=2) + mapview(x)
```

#### httr and jsonlite

In R, httr and jsonlite packages are used to consume the API's that provide data in json format.

**httr** httr provides us with an HTTP client to access the API with GET/POST methods, passing query parameters, and verifying the response with regard to the data format.

**jsonlite** This R package is used to convert the received json format to readable R object or data frame. `jsonlite` can also be used to convert R objects or a data frame to a json format data type.

Along with these two packages, `rlist` in R can be used to perform additional manipulation on the received json response. `list.stack` and `list.select` are two important methods exposed by `rlist` that can be used to get parsed json data into a `tibble`.

To learn about these (and other) packages, type `?httr`, `?jsonlite` and `?rlist` in the console of Rstudio to view the documentation.

Here we see an example using a JSON API from the world bank that uses a GET request below - we can see a couple ways of passing request parameters - first we'll embed the query directly in the URL for the request itself:

```{r}
#| message: false
#| warning: false
library(httr) 
library(jsonlite) 
library(dplyr)
library(data.table)

jsonResponse <- httr::GET("http://api.worldbank.org/country?per_page=10&region=OED&lendingtype=LNX&format=json")
```

We can also generate the query by passing it in a separate list we create and adding as a query parameter in the GET request

```{r}
query<-list(per_page="10",region="OED",lendingtype="LNX",format="json")
jsonResponse <- httr::GET("http://api.worldbank.org/country",query=query)
```

The `jsonlite` package can be used to parse our response which we indicated above to return as json. Why does the attempt below return an error?

```{r}
#| eval: false
#| warning: false
df <- jsonlite::fromJSON(jsonResponse)
```

HINT: what type of object is `jsonResponse` above?

```{r}
typeof(jsonResponse)
```

We can also get information about the response using `httr`:

```{r}
http_type(jsonResponse)
```

We have a list - let's take a look to figure out to pull out what we want into a data frame:

```{r}
names(jsonResponse)
```

It seems like `content` is what we want, how do we pull out?

```{r}
#| eval: false
#| warning: false
df <- jsonlite::fromJSON(jsonResponse$content)
```

Still doesn't work - we need to do some further parsing of this response using `content` in `httr` which we can then pass to `jsonlite` and the `fromJSON` function to get data into a data frame

```{r}
jsonResponseParsed <- httr::content(jsonResponse, as="parsed")
is.list(jsonResponseParsed)
names(jsonResponseParsed[[1]])
names(jsonResponseParsed[[2]])
#Hmmm
is.list(jsonResponseParsed[[2]][[1]]) 
names(jsonResponseParsed[[2]][[1]])
# now we're getting somewhere...
names(jsonResponseParsed[[2]][[2]])
```

We convert the parsed json response list to data table using `lapply` and `rbindlist`:

```{r}
df <- lapply(jsonResponseParsed[[2]],as.data.table) 
typeof(df) # still a list - one more step
dt <- rbindlist(df, fill = TRUE)
```

Some APIs like [GitHub](%22https://api.github.com%22) are easier to pull directly into a dataframe using `fromJSON`

```{r}
myGitHubRepos <- fromJSON("https://api.github.com/users/mhweber/repos")

# It returns 79 variables about my repos - we can use select to just get the ones we want to see
myGitHubRepos <- myGitHubRepos |> 
  dplyr::select(name, stargazers_count, watchers_count, language, has_issues, forks_count)
head(myGitHubRepos)
```

### Examples in Python (separate notebook in binder)

## Data access summary 
This just builds on examples for [Working with Geospatial Hydrologic Data Using Web Services (R) for IOW Webinar](https://mikejohnson51.github.io/IOW2022_R/slides.html)

### AOI

AOI is _NOT_ on CRAN but provides interfaces for geocoding and retrieving spatial boundaries (e.g state, county, and country). When working with web-based,  subsetting services, the ability to quickly define common features is really convenient.

We'll see through these examples that the concept of AOI is key to requesting subsets of data. Further well see that a reproducible way of generating these is a quic kway to make the sytanx of disparate packages more aligned.


```{r}
# remotes::install_github("mikejohnson51/AOI")
library(AOI)

# state
aoi_get(state = "OR") |> mapview()

# county
aoi_get(state = "OR", county = "Benton") |> mapview()

# country
aoi_get(country = "Ukraine") |> mapview()

# geocoding
geocode("LaSells Stewart Center Corvallis", pt = TRUE) |> mapview()
```

### nhdplusTools

We'll hear a lot about Dave's package [nhdplusTools](https://usgs-r.github.io/nhdplusTools/index.html) - it's primarily a R package for working with the NHD and the NHD data model. The package covers 5 primary use topics that include:

1. [data access](https://usgs-r.github.io/nhdplusTools/reference/index.html#data-access)
2. [data discovery and subsetting (via web services)](https://usgs-r.github.io/nhdplusTools/reference/index.html#discovery-and-subsetting)
3. [Indexing and Network Navigation](https://usgs-r.github.io/nhdplusTools/reference/index.html#indexing-and-network-navigation)
4. [Network Navigation](https://usgs-r.github.io/nhdplusTools/reference/index.html#network-navigation)
5. [Network Attributes](https://usgs-r.github.io/nhdplusTools/reference/index.html#network-attributes)

Given the focus here on geospatial data via web services, we will look at functions supporting use case 2.

```{r}
# install.packages('nhdplusTools')
library(nhdplusTools)

grep("get_", ls("package:nhdplusTools"), value = TRUE)
```

#### Basic Use

Let's get data for an AOI around Corvalis. We use `AOI::aoi_get` to get the OpenStreetMap representation of Corvallis, and initialize an empty list to populate: 

```{r}
corvallis     = list()
corvallis$AOI = aoi_get("Corvallis, OR")
```

Populate our Corvallis list with various elements we pull from `get_` functions in `nhdplusTools`:
```{r}
corvallis$nhdplus      <- get_nhdplus(AOI = corvallis$AOI) 
corvallis$waterbodies  <- get_waterbodies(AOI = corvallis$AOI) 
corvallis$gages        <- get_nwis(AOI = corvallis$AOI) 
corvallis$huc12        <- get_huc12(AOI = corvallis$AOI) 

# How many features per entry?
sapply(corvallis, nrow)
```

```{r}
mapview(corvallis)
```

In both the NHDPlus data model, and the emerging international OGC standard for representing surface water features [HY_features](https://docs.opengeospatial.org/is/14-111r6/14-111r6.html) it's recognized that hydrologic entities can be "realized" in a number of ways. 

For example the holistic notion of a 'catchment' can be realized as a 'divide', 'flowpath', 'outlet', or other representation. The `nhdplusTools::get_nhdplus` functions support getting 3 realizations of the NHD features that are consistent with the NHDPlus Data model (`outlet`, `flowline`, `catchment`, and `all`).

```{r}
corvallis <- get_nhdplus(aoi_get("Corvallis, OR"), realization = "all")
mapview(corvallis)
```

### dataRetrival

The USGS supported [dataRetrieval](https://usgs-r.github.io/dataRetrieval/) package provides retrieval functions for USGS and EPA hydrologic and water quality data. Recently a Network Linked Data Index (NLDI) client was also added to the [package](http://usgs-r.github.io/dataRetrieval/articles/nldi.html).

All sorts of dataRetrieval resources:
[Home page](https://rconnect.usgs.gov/dataRetrieval/)
[Tutorial for National Water Quality Monditoring Conference - Part 1](https://rconnect.usgs.gov/NMC_dataRetrieval_1/dataRetrieval_1.html#/title-slide)
[Tutorial for National Water Quality Monditoring Conference - Part 2](https://rconnect.usgs.gov/NMC_dataRetrieval_2/dataRetrieval_2.html#/title-slide)

#### Basic Use: Get streamflow Data
What were the two gages we picked up geolocating Corvallis?

```{r}
corvallis$gages
```

```{r}
#install.package('dataRetrieval')
library(dataRetrieval)
library(ggplot2)

# Supply gauge ids from Corvallis example
# Parameter codes can be found with (`dataRetrieval::parameterCdFile`)
# "00060" is stream flow

# we need to bring gages back to corvallis after our last operation
corvallis$AOI = aoi_get("Corvallis, OR")
corvallis$gages  <- get_nwis(AOI = corvallis$AOI)
flows = readNWISdv(siteNumbers = corvallis$gages$site_no, parameterCd = "00060") |> 
  renameNWISColumns()

ggplot(data = flows) + 
  geom_line(aes(x = Date, y = Flow)) + 
  facet_wrap('site_no')
```

#### water quality
We can also get water quality information - see the [NWQMC DataRetrieval Tutorial Part 1 for all the details](https://rconnect.usgs.gov/NMC_dataRetrieval_1/dataRetrieval_1.html#/title-slide)

```{r}
readWQPdata(statecode = "OR",
            countycode = "Benton",
            characteristicName = "Phosphorus",
            sampleMedia = "Water",
            dataProfile = "resultPhysChem") 
```

Let's explore phosphorus data in Benton County
```{r}
benton_phos <- readWQPsummary(statecode = "OR",
                          countycode="Benton",
                          sampleMedia = "Water",
                          characteristicName = "Phosphorus")

names(benton_phos) 

```
Apply some `dplyr` functions to group site information
```{r}
benton_phos_summary <- benton_phos |> 
  rename(Site = MonitoringLocationIdentifier) |>
  mutate(Lat = as.numeric(MonitoringLocationLatitude),
         Lon = as.numeric(MonitoringLocationLongitude)) |> 
  group_by(Site, Lat, Lon) |> 
  summarise(min_year = min(YearSummarized),
            max_year = max(YearSummarized),
            count = sum(ResultCount)) |> 
  mutate(POR = max_year - min_year) |> 
  arrange(desc(count)) |> 
  ungroup()

benton_phos_summary
```

View with `mapview`- first we need to make our results an `sf` data frame for mapview to use.  Easy!
```{r}
results_sf <- benton_phos_summary  |> 
  sf::st_as_sf(coords = c("Lon", "Lat"), crs = 4269, remove = FALSE)
mapview(results_sf)
```

### NLDI client

The hydro [Network-Linked Data Index (NLDI)](https://labs.waterdata.usgs.gov/about-nldi/index.html) is a system that can index spatial and river network-linked data and navigate the river network to allow discovery of indexed information.

The NLDI service requires the following:

1. A starting entity (`comid`, `nwis`, `wqp`, `location` and more). 
 - The "more" includes the ever-growing set of resources available in the NLDI service that can be accessed with the `get_nldi_sources` function.

```{r}
DT::datatable(get_nldi_sources())
```

2. A direction to navigate along the network
  - UM: upper mainstem
  - UT: upper tributary
  - DM: downstream mainstem
  - DD: downstream divergence

3. Features to find along the way!
  - Any of the above features (e.g. `get_nldi_sources`)
  - flowlines
  - basin

#### Basic use: NLDI

The NLDI client in `dataRetrival` operates by speficing the origin, the navigation mode, and the features to find. In the following example, we want to navigate along the upstream tributary of COMID 101 (up to 1000 km) and find all flowline features, nwis gages, and the contributing drainage area. 

```{r}
nldi = findNLDI(comid= 23763529, 
             nav= "UT", 
             find= c("flowline", "nwis", 'basin'),
             distance_km = 1000)

sapply(nldi, nrow)
```

```{r}
mapview(nldi)
```

### nwmTools

While `dataRetrival` provides retrieval functions for observed data, there is an increasing amount of simulated data available for use. Once example is the 3 retrospective simulations of the [National Water Model](https://water.noaa.gov/about/nwm). This data certainly falls under the umbrella of ["big data"](https://www.cuahsi.org/about/news/big-data-dreaming-a-42-year-conus-hydrologic-retrospective) and can be difficult to work with given each hourly file (for up to 42 years!) is stored in separate, cloud based files.

[`nwmTools`](https://mikejohnson51.github.io/nwmTools/) provides retrieval functions for these retrospective stream flow data from the NOAA National Water Model reanalysis products:

#### Retrival


```{r}
# remotes::install_github("mikejohnson51/nwmTools")
library(nwmTools)

nwis = readNWISdv(site = gsub("USGS-", "", nldi$UT_nwissite$identifier[8]), 
                  param = "00060") |> 
  renameNWISColumns()

nwm = readNWMdata(comid = nldi$UT_nwissite$comid[8])
```

#### Aggregation

The timestep of the retrospective NWM data is hourly. In many cases, its more desirable to have the time step aggregated to a different temporal resolution. nwmTools provides a family of aggregation functions that allow you to specify the aggregation time period, and the aggregation summary statistic

```{r}
grep('aggregate_', ls("package:nwmTools"), value = TRUE)

```

```{r}
# Core function
nwm_ymd  = aggregate_ymd(nwm, "mean")
# User defined function
seasonal = aggregate_s(nwm, fun = function(x){quantile(x,.75)})
# Multiple functions
month    = aggregate_m(nwm, c('mean', "max"))

a <-ggplot(data = nwis) + 
  geom_line(aes(x = Date, y = Flow * 0.028316846592)) + 
  labs(title = "NWIS Data") 
b <-ggplot(data = nwm_ymd) + 
  geom_line(aes(x = ymd, y = flow_cms_v2.1)) + 
  labs(title = "NWM Daily Average")  
c <-ggplot(data = seasonal) + 
  geom_col(aes(x = season, y = flow_cms_v2.1)) + 
  labs(title = "NWM Seasonal 75%") 
d <-ggplot(data = month) + 
  geom_col(aes(x = as.factor(month), y = max)) +
  geom_col(aes(x = as.factor(month), y = mean), fill = "red") +
  labs(title = "NWM Monthly Mean/Maximum")
cowplot::plot_grid(a,b,c,d)
```

### opendap.catalog

One of the biggest challenges with Earth System and spatial research is extracting data. These challenges include not only finding the source data but then downloading, managing, and extracting the partitions critical for a given task.

Services exist to make data more readily available over the web but introduce new challenges of identifying subsets, working across a wide array of standards (e.g. non-standards), all without alleviating the challenge of finding resources.

In light of this, [opendap.catalog](https://mikejohnson51.github.io/opendap.catalog/) provides 4 primary services. 

Learn more about [OPenDAP here](https://www.earthdata.nasa.gov/engage/open-data-services-and-software/api/opendap)

#### Generalized space (XY) and Time (T) subsets for remote and local NetCDF data with dap()
 - Handles data streaming, projection, and cropping. Soon to come is masking capabilities.

```{r}
# remotes::install_github("mikejohnson51/opendap.catalog")
library(opendap.catalog)

AOI = AOI::aoi_get(state = "OR", county = "Benton")

dap <- dap(URL = 'http://cida.usgs.gov/thredds/dodsC/prism',
           AOI = AOI,
           varname = 'ppt',
           startDate = "2005-01-01",
           endDate = "2005-01-31")

str(dap, max.level = 1)

plot(terra::rast(dap))
```

#### A catalog of `r nrow(opendap.catalog::params)` web resources (as of 06/2022)

```{r}
dplyr::glimpse(opendap.catalog::params)
```

With 14,160 web resources documented, there are simply too many resources to search through by hand unless you know exactly what you want. This voids the possibility of serendipitous discovery. So, we have added a generally fuzzy search tool to help discover datasets.

Say you want to find what resources there are for daily rainfall? `search` and `search_summary` can help:

```{r}
search("daily precipitation") |> 
  search_summary() |> 
  DT::datatable()

(s = search('gridmet daily precipitation'))
```

#### Pass catalog elements to the generalized toolsets:

Now that you know the precipitation product you want, lets say we want to look at precipitation in Benton County in April 2022:

```{r}
(s = opendap.catalog::search('gridmet daily precipitation'))

(OR_rain = dap(catalog = s, 
             AOI = AOI::aoi_get(state = "OR"), 
             startDate = "2022-04-01",
             endDate   = "2022-09-30"))

{
  plot(sum(OR_rain[[1]]) * 0.001, 
       main = "Meters of rainfall: Oregon April 2022")
}
```

#### Tiled data streams

One of the key features of `dap` is the capacity to make requests over tiled resources. Some resources (like [MODIS](https://modis.gsfc.nasa.gov/)) are tiled in space, and some (like [MACA](https://www.climatologylab.org/maca.html), and [LOCA](https://loca.ucsd.edu/what-is-loca/)) are tiled in time. Below we see an example of how a single `dap` call can consolidate a request that covers multiple spatial tiles:

This one isn't working at the moment - not sure why...
```{r}
(s = opendap.catalog::search('mod16a2v006 PET'))
# dap = dap(
#     catalog = s,
#     AOI = AOI::aoi_get(state = "OR", county = "Benton"),
#     startDate = "2010-01-01",
#     endDate   = "2010-01-31"
#   )
# 
# plot(dap)
```

### Zonal analysis

Often out goal when acquiring spatial dat is producing area-based summaries, particularly for watershed analyses.

[`zonal`](https://github.com/NOAA-OWP/zonal) is a package that wraps the excellent [`exactextractr`](https://isciences.gitlab.io/exactextractr/) package with some added performance and convenience for these types of tasks. 

To use zonal, you must supply:

1. a raster dataset (SpatRaster)
2. a geometry set to summarize over
3. a summary function (similar to `nwmTools::aggregate_*`)
4. The column with a unique identifier for each polygon object, and whether the summary data should be joined (`join  = TRUE`) to the original summary data.

### zonal

Often getting spatial data is not the end goal and in many cases producing area based summaries is critical.

[`zonal`](https://github.com/NOAA-OWP/zonal) is a package that wraps the excellent [`exactextractr`](https://isciences.gitlab.io/exactextractr/) package with some added performance and convenience for these types of tasks. To use zonal, you must supply a raster dataset (SpatRaster), a geometry set to summarize over, and a summary functions (similar to `nwmTools::aggregate_*`). Lastly, you must provide the column with a unique identifier for each polygon object, and whether the summary data should be joined (`join  = TRUE`) to the original summary data.


```{r}
# remotes::install_github("NOAA-OWP/zonal")
library(zonal)

# Summary options
zonal::ee_functions()
zonal::weight_functions()
```

#### Basic Use

```{r}
library(zonal)

OR_counties <- AOI::aoi_get(state = "OR", county = "all")
system.time({
summary_data = execute_zonal(OR_rain, 
                             geom = OR_counties, 
                             fun = "sum", 
                             ID = "fip_code", 
                             join = TRUE)
})
```

```{r}
plot(summary_data[grepl("sum", names(summary_data))], border = NA,
     max.plot = 20)
```

### Web data without a "service"

Often the needed data is not delivered by a service or function. This does not mean we are out of luck, but rather that we have to do a little of the "heavy lifting" ourselves. This final section highlights how to vector and raster data from a URL using the http/https and s3 protocols.

These approaches are advantageous when you don't want to download data locally for analysis.


#### Access Raster Data

GDAL provides a number of [Virtual File System](https://gdal.org/user/virtual_file_systems.html) drivers to read compressed and network data. 

Three of the most useful for common spatial data tasks include:

- **vsizip** --> file handler that allows reading ZIP archives on-the-fly without decompressing them beforehand.

- **vsicurl** --> A generic file system handler exists for online resources that
  do not require particular signed authentication schemes

- **vsis3** --> specialized into sub-filesystems for commercial cloud storage services (e.g. AWS, also see  /vsigs/, /vsiaz/, /vsioss/ or /vsiswift/).

#### Basic Usgage

Duke store [VRT](https://gdal.org/drivers/raster/vrt.html) files for the POLARIS soils dataset. `terra` can generate pointers to these datasets -  however data is not extracted until an operation is called on the data. As seen above `dap` provides simplified access to extract subsets of data for OpenDap and VRT endpoints.

```{r}

terra::rast('/vsicurl/http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/vrt/alpha_mean_0_5.vrt') 

opendap.catalog::dap('/vsicurl/http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/vrt/alpha_mean_0_5.vrt', AOI = AOI::aoi_get(state = "OR", county = "Benton")) |> 
  plot()
```

#### Transect example

Here is an example where a global elevation dataset can quickly be used in conjunction with the `NLDI` client to plot the elevation transect of a the Calapooyia River near here:

```{r}

d = dataRetrieval::findNLDI(loc = geocode("Pioneer Park Brownsville, Oregon", pt = TRUE),
             nav = "UM", 
             find = c("flowlines", "basin")) 

mapview::mapview(d)

COP30 = opendap.catalog::dap("/vsicurl/https://opentopography.s3.sdsc.edu/raster/COP30/COP30_hh.vrt", 
            AOI = d$basin)

transect = terra::extract(COP30, terra::vect(sf::st_union(d$UM_flowlines))) |> 
  dplyr::mutate(location = c(n():1))

ggplot2::ggplot(transect, aes(x = location, y = COP30_hh)) +
    geom_line() +
    geom_smooth()
```


#### Access Vector Data

**Basic Usgage: TIGER lines** 
Since the base data reader of sf is GDAL the vsi capabilities also apply to vector data! Note that the `tigris` and `tidycensus` packages are the recommended way to access US Census data, this is simply by way of example. The added challenge typically is that vector data (especially shapefile data) is generally stored in zip files. This adds an extra layer of complexity when remotely reading these datasets. 

In this example we look at the US Census Bureaus FTP server for TIGER road lines. The data is stored by by 5 digit (county) FIP code in zip files. To access this data we need to identify the FIP code of interest and then chain a `vsizip` with a `vsicurl` call.

```{r}
AOI  = aoi_get(state = "OR", county = "Benton")
file = paste0('tl_2021_',AOI$fip_code, '_roads')
url  = paste0('/vsizip/{/vsicurl/https://www2.census.gov/geo/tiger/TIGER2021/ROADS/', file, '.zip}/',file,'.shp')

system.time({
  roads  = sf::read_sf(url)
})

mapview::mapview(roads)
```

For those curious about the generalization of this, we can see that any GDAL backed software (here terra) can utilize the same URL:

```{r}
system.time({
  roads2 = terra::vect(url)
})

plot(roads2)
```

## Use Cases

### Hydro Addressing

Finding where along a river system some spatial data lies is a key use case for many modeling and analysis tasks. A full discussion is available in the [`nhdplusTools` indexing and referencing vignette.](https://doi-usgs.github.io/nhdplusTools/articles/indexing.html)

We need two inputs. Lines to index to and points that we want addresses for. With these inputs, [`nhdplusTools`](https://doi-usgs.github.io/nhdplusTools/reference/index.html#indexing-and-network-navigation) (and soon [`hydroloom`](https://doi-usgs.github.io/hydroloom/reference/index.html#indexing-and-linear-referencing)) supports generation of hydro addresses with `get_flowline_index()`.

The example below is as simple as possible. `get_flowline_index()` has a number of other capabilities, such as increased address precision and the ability to return multiple nearby addresses, that can be found in the function documentation.

```{r}
#| warning: false
source(system.file("extdata/new_hope_data.R", package = "nhdplusTools"))

fline <- sf::st_transform(sf::st_cast(new_hope_flowline, "LINESTRING"), 4326)

point <- sf::st_sfc(sf::st_point(c(-78.97, 35.916)),
                    crs = 4326)

(address <- nhdplusTools::get_flowline_index(fline, point))

plot(sf::st_geometry(fline[fline$COMID %in% address$COMID,]))+
  plot(point, add = TRUE)
```

A natural next step once we've found a hydro address is to search upstream or downstream from the location we found. [`nhdplusTools`](https://doi-usgs.github.io/nhdplusTools/reference/index.html#network-navigation) and soon [`hydroloom`](https://doi-usgs.github.io/hydroloom/reference/index.html#network-navigation-and-accumulation) offer a few upstream / downstream search functions. Here we'll show a couple from `nhdplusTools`.

```{r}
#| warning: false
up <- nhdplusTools::get_UT(fline, address$COMID)
um <- nhdplusTools::get_UM(fline, address$COMID)
dn <- nhdplusTools::get_DD(fline, address$COMID)
dm <- nhdplusTools::get_DM(fline, address$COMID)

plot(sf::st_geometry(fline), col = "grey", lwd = 0.5)
plot(sf::st_geometry(fline[fline$COMID %in% c(up, dn),]),
     add = TRUE)
plot(sf::st_geometry(fline[fline$COMID %in% c(um, dm),]),
     col = "blue", lwd = 2, add = TRUE)
plot(point, cex = 2, lwd = 2, add = TRUE)

```

The ability to navigate up or down a mainstem rather than all connected tributaries and diversions, requires some attributes that identify primary up and downstream paths. Without those paths, navigation is still possible but the "main" path navigation method is not. Below, a new function available in `hydroloom`, `navigate_network_dfs()` is shown.

```{r}
# remotes::install_github("DOI-USGS/nhdplusTools@2cb81da")
#| warning: false
net <- hydroloom::add_toids(sf::st_drop_geometry(fline), 
                            return_dendritic = FALSE)

up <- hydroloom::navigate_network_dfs(net, address$COMID, direction = "up")
dn <- hydroloom::navigate_network_dfs(net, address$COMID, direction = "dn")

plot(sf::st_geometry(fline), col = "grey", lwd = 0.5)
plot(sf::st_geometry(fline[fline$COMID %in% c(unlist(up), unlist(dn)),]),
     add = TRUE)
plot(point, cex = 2, lwd = 2, add = TRUE)
```

#### BONUS Use Case

Going forward, linear referencing with the predecessor to the NHD will be to 
a "mainstem" identifier. A "mainstem id" is a Uniform Resource Identifier that 
works like a Digital Object Identifier. e.g. `https://geoconnex.us/ref/mainstems/1435189` 
is the Calapooia River.

If we know the uri ahead of time, we could look it up like this:

```{r}
calapooia <- sf::read_sf("https://geoconnex.us/ref/mainstems/1435189")
names(calapooia)
mapview::mapview(calapooia)
```
In the future, any data along the Calapooia would just link to that URI and we
would know that it is supposed to be linked to that river. 

If we don't know what river our data is on, we can find the URI from a lookup table.
The mainstem system is brand new. Over time, mainstem lookup tables will become 
more ubiquitous. Stay tuned!!

```{r}

point <- sf::st_sfc(sf::st_point(c(-123.149, 44.565)),
                    crs = 4326)

comid <- nhdplusTools::discover_nhdplus_id(point)

mainstem_lookup <- readr::read_csv("https://github.com/internetofwater/ref_rivers/releases/download/v2.1/nhdpv2_lookup.csv")

mainstem_lookup$uri[mainstem_lookup$comid == comid]

```


### Catchment Characteristics and Accumulation

[`nhdplusTools`](https://doi-usgs.github.io/nhdplusTools) (only in the latest `hydroloom` branch) and [`StreamCatTools`](https://usepa.github.io/StreamCatTools) can be used to retrieve a wide range of catchment characteristics. See the [StremCatTools Vignette](https://usepa.github.io/StreamCatTools/) for further details and examples.

```{r}
#| warning: false
library(nhdplusTools)
# remotes::install_github("USEPA/StreamCatTools")
library(StreamCatTools)
nhdplusTools::nhdplusTools_data_dir(tempdir())
cat <- sf::st_transform(new_hope_catchment, 4326)

streamcat_chars <- StreamCatTools::sc_get_params(param='name')

nawqa_chars <- nhdplusTools::get_characteristics_metadata()

StreamCatTools::sc_fullname("pctimp2016")

nawqa_chars$description[nawqa_chars$ID == "CAT_TWI"]

twi <- nhdplusTools::get_catchment_characteristics("CAT_TWI", cat$FEATUREID)

twi <- dplyr::rename(twi, CAT_TWI = "characteristic_value")

imp <- StreamCatTools::sc_get_data("pctimp2016", 'catchment', cat$FEATUREID)

cat <- dplyr::left_join(cat, imp, by = c("FEATUREID" = "COMID")) 
  # dplyr::left_join(twi, by = c("FEATUREID" = "comid"))

# plot(cat['CAT_TWI'])
plot(cat['PCTIMP2016CAT'])
```

With the two catalogs of characteristics above, we have access to nearly any characteristic imaginable and both even offer downstream accumulations of the variables. However, sometimes it is necessary to sample and accumulate data not already available. `nhdplusTools` supports accumulating drainage area and length and `hydroloom`, going forward, supports downstream accumulation more generically. Here, we'll just use one of the examples pulled above to illustrate how this works.

```{r}
library(dplyr)
accum_df <- sf::st_drop_geometry(fline) |>
  dplyr::select(COMID, FromNode, ToNode, Divergence, AreaSqKM) |>
  hydroloom::add_toids() |>
  dplyr::left_join(dplyr::select(cat, -AreaSqKM), by = c("COMID" = "FEATUREID")) |>
  sf::st_sf() |>
  dplyr::mutate(area_weighted_PCTIMP2016CAT = 
           ifelse(is.na(PCTIMP2016CAT) & AreaSqKM == 0, 
                  yes = 0, no = AreaSqKM * PCTIMP2016CAT))

accum_df <- accum_df |>
  dplyr::mutate(tot_PCTIMP2016CAT = 
           hydroloom::accumulate_downstream(accum_df, "area_weighted_PCTIMP2016CAT") / 
           hydroloom::accumulate_downstream(accum_df, "AreaSqKM"))

plot(cat['PCTIMP2016CAT'])
plot(accum_df['tot_PCTIMP2016CAT'])
```

### Spatial Data Aggregation and Resampling

Numerous remotely sensed and modeled datasets are available spatially and temporally continuous coverages that can be accessed and processed without first downloading the data. Working with these datasets can be challenging due to data scale and access nuances. The [`opendap.catalog`](https://mikejohnson51.github.io/opendap.catalog/) R package provides a unified catalog of such data sources that can be accessed with both R and Python clients. `opendap.catalog` provides a simple way to discover and retrieve subsets of large XY[Z][T] datasets.

The demo below shows how to access the catalog and pull a subset of one data source. There's much much more available in the catalog, including some advanced tiling of datasets such as MODIS. 

```{r}
library(opendap.catalog)
glimpse(opendap.catalog::params)
search("daily precipitation") |> 
  search_summary() |> 
  DT::datatable()
(s = search('gridmet daily precipitation'))
(harvey = dap(catalog = s, 
             AOI = aoi_get(state = c("TX", "LA")), 
             startDate = "2017-08-17",
             endDate   = "2017-09-03"))
plot(harvey[[1]])
```

Building on this simple data access, we can use the `zonal` package to create spatial summary statistics. Similar summaries can also be created with the `gdptools` python package relying on the [opendap.catalog](https://gdptools.readthedocs.io/en/latest/index.html) as a source of web availabledata. 


```{r}
# remotes::install_github("NOAA-OWP/zonal")
library(zonal)

harvey_counties = aoi_get(state = c("TX", "LA"), county = "all")

system.time({
summary_data = execute_zonal(harvey, 
                             geom = harvey_counties, 
                             fun = "sum", 
                             ID = "fip_code", 
                             join = TRUE)

plot(summary_data[grepl("sum", names(summary_data))], border = NA,
     max.plot = 16)
})

```

## R and Python Interoperability

We can us[Reticulate](https://rstudio.github.io/reticulate/index.html) to inter-operate easily between R and Python within RStudio and share objects between languages

```{r}
library(reticulate)
# reticulate::install_miniconda()
# create a new environment conda environment 
# conda_create("r-reticulate")

# install NumPy and SciPy
# conda_install("r-reticulate", "scipy")
# conda_install("r-reticulate", "numpy")
# conda_install("r-reticulate", "pandas")
# indicate that we want to use a specific condaenv
use_condaenv("r-reticulate")
np <- import("numpy")
print(np$version$full_version)
```

Simple examples from [reticulate - calling Python](https://rstudio.github.io/reticulate/articles/calling_python.html):

Import a module and call a function from Python:

```{r}
os <- import("os")
os$listdir(".")
```

Object conversion - When Python objects are returned to R they are converted to R objects by default - but you can deal in native Python types by choosing `convert=FALSE` in the import function

```{r}
# import numpy and specify no automatic Python to R conversion
np <- import("numpy", convert = FALSE)

# do some array manipulations with NumPy
a <- np$array(c(1:4))
print (a)

sum <- a$cumsum()

# convert to R explicitly at the end
print (py_to_r(a))

py_to_r(sum)
```

Passing Dataframes between Python and R:

```{r}
pd <- import("pandas")
penguins <- pd$read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv")
dplyr::glimpse(penguins)
```

Or like this:

```{python}
import pandas as pd
penguins =pd.read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv")
print(penguins.head())
```

Call Python from R:

```{r}
penguins <- py$penguins
dplyr::glimpse(penguins)

```

Call R from Python:

```{r}
data(iris)
iris <- iris |> 
  dplyr::filter(Petal.Length > 3)
```

```{python}
print(r.iris.head())
print(r.iris['Petal.Length'].min())
```

## Resources

### General

-   [Awesome Geospatial](https://github.com/sacridini/Awesome-Geospatial)

### R

-   [Hydroinformatics in R](https://vt-hydroinformatics.github.io/): Extensive Notes and exercises for a course on data analysis techniques in hydrology using the programming language R
-   [Spatial Data Science by Edzar Pebesma and Roger Bivand](https://r-spatial.org/book/)
-   [Geocomputation with R](https://r.geocompx.org/)
-   [r-spatial](https://github.com/r-spatial): Suite of fundamental packages for working with spatial data in R
-   [Working with Geospatial Hydrologic Data Using Web Services (R)](https://mikejohnson51.github.io/IOW2022_R/slides.html)
-   [Accessing REST API (JSON data) using httr and jsonlite](https://rstudio-pubs-static.s3.amazonaws.com/480665_ba2655419209496dbb799f1c7d050673.html)

### Python

-   [Datashader](https://datashader.org/): Accurately render even the largest data
-   [GeoPandas](https://geopandas.org/en/stable/index.html)
-   [HyRiver](https://docs.hyriver.io/index.html): a suite of Python packages that provides a unified API for retrieving geospatial/temporal data from various web services
-   [Python Foundation for Spatial Analysis](https://courses.spatialthoughts.com/python-foundation.html)
-   [Python for Geographic Data Analysis](https://pythongis.org/index.html)
-   [gdptools](https://gdptools.readthedocs.io/en/latest/) A Python package for grid- or polygon-polygon area-weighted interpolation statistics
-   [Intro to Python GIS](https://automating-gis-processes.github.io/CSC18/lessons/L2/geopandas-basics.html)
-   [xarray](https://xarray.pydata.org/en/stable/): An open-source project and Python package that makes working with labeled multi-dimensional arrays simple, efficient, and fun!
-   [rioxarray](https://corteva.github.io/rioxarray/stable/index.html): Rasterio xarray extension.
-   [GeoPandas](https://geopandas.org/en/stable/): An open-source project to make working with geospatial data in python easier.
-   [OSMnx](https://github.com/gboeing/osmnx): A Python package that lets you download and analyze geospatial data from OpenStreetMap.
-   [Xarray Spatial](https://xarray-spatial.org/master/index.html): Implements common raster analysis functions using `numba` and provides an easy-to-install, easy-to-extend codebase for raster analysis.
