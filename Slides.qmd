---
title: "Introduction"
format: 
  html:
    toc: true
    toc-depth: 4
    toc-location: left
    self-contained: true
    code-link: true
    code-fold: true
editor: visual
date: 2023-06-05
---

## Introduction

Housekeeping, introductions

*The views expressed in this presentation are those of the authors and do not necessarily represent the views or policies of the U.S. Environmental Protection Agency or the U.S.G.S.*

*This land is ancestral and unceded land of the Chepenafa (Mary’s River) Band of the [Kalapuya](https://libraryguides.lanecc.edu/kalapuya) (pronounce “cal-uh-poo-yuh”), who have stewarded this beautiful area for generations.  The descendants of the Kalapuya People are now part of the federally recognized [Confederated Tribes of Grand Ronde](https://www.grandronde.org/) and [Confederated Tribes of the Siletz](https://www.ctsi.nsn.us/) of Oregon. The peak you see from numerous vantage points around Corvallis is Mary's Peak, which the Kalapuya called tcha Timanwi, or 'place of spiritual power', and is the highest peak in the Oregon Coast Range (4,097 feet).*

## Key Concepts Across Both languages

### Core Libraries

-   Spatial data structures across languages and applications is primarily organized through [OSgeo](https://www.osgeo.org) and [OGC](https://www.ogc.org)) *and* a few core libraries underpin spatial libraries in programming languages and in software applications(R, Python, QGIS, ArcPro)!

These libraries include:

-   **PROJ** --\> Spatial projections, transformations

-   **GEOS** --\> Geometry operations (measures, relations)

-   **GDAL** --\> Raster and feature abstraction and processing (read, write)

-   **NetCDF** --\> Multidimensional (XYZT) data abstraction (read, write)

### Requesting web services (APIs)

More and more data is available as web services, and the data is stored in a remote server in JSON, XML, HTML format which is accessible through API's.

**JSON** (JavaScript Object Notation) is a lightweight data-interchange format which is easy for machines to generate and parse; easy for humans to write and read.

Sample JSON format : { "data": "Click Here", "size": 36, "style": "bold", "name": "text1", }

**API** - An Application Programming Interface (API) takes a structured request from an application and returns structured results from the host application. With a **Rest API** you're getting a representation of the requested data stored in a server. A **Rest API** also is what we call 'stateless', which means a server doesn't store any data between requests from clients.

**Rest APIs** access data through **uniform resource identifiers (URIs)**, which are essentially a string of characters that identify a specific resource. The type of URI used by a **Rest API** is a **uniform resource locator (URL)**.

**HTTP** clients are used for accessing the API. **HyperText Transfer Protocol (HTTP)** enables communication between the client and server using **HTTP** methods. If we want to access or manipulate resources a **Rest API** uses specific request verbs we need to become familiar with:

-   GET: used to acquire data from a database

-   POST: used to add data to a database

-   PUT: update the data in a database

-   DELETE: delete data in a database

### Geospatial data representation fundamentals: simple features and geospatial grids

-   \***Vector** data are comprised of points, lines, and polygons that represent discrete spatial entities, such as a river, watershed, or stream gauge.

-   **Raster** data divides spaces into rectilinear cells (pixels) to represent spatially continuous phenomena, such as elevation or the weather. The cell size (or resolution) defines the fidelity of the data.

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/09-vec-raster.jpg")
```

For *Vector* data Simple Features (officially Simple Feature Access) is both an OGC and International Organization for Standardization (ISO) standard that specifies how (mostly) two-dimensional geometries can represent and describe objects in the real world. Simple features includes:

-   a class hierarchy
-   a set of operations
-   binary and text encodings

It describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.

It outlines how the spatial elements of POINTS (XY locations with a specific coordinate reference system) extend to LINES, POLYGONS and GEOMETRYCOLLECTION(s).

The "simple" adjective also refers to the fact that the line or polygon geometries are represented by sequences of points connected with straight lines that do not self-intersect.

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/09-sf-model.png")
```

#### Simple and valid geometries and ring direction

This breakdown of simple features follows for the most part [this section in Spatial Data Science](https://r-spatial.org/book/03-Geometries.html) For *linestrings* to be considered *simple* they must not self-intersect:

```{r}
library(sf)
(ls <- st_linestring(rbind(c(0,0), c(1,1), c(2,2), c(0,2), c(1,1), c(2,0))))
```

```{r}
#| message: false
#| echo: false
#| error: false
plot(ls)
```

```{r}
#| message: false
#| echo: false
#| error: false
c(is_simple = st_is_simple(ls))
```

For *polygons* several other conditions have to be met to be *simple*:

-   polygon rings are closed (the last point equals the first)
-   polygon holes (inner rings) are inside their exterior ring
-   polygon inner rings maximally touch the exterior ring in single points, not over a line
-   a polygon ring does not repeat its own path
-   in a multi-polygon, an external ring maximally touches another exterior ring in single points, not over a line

*z* and *m* coordinates As well as having the necessary X and Y coordinates, single point (vertex) simple features can have:

-   a Z coordinate, denoting altitude, and/or
-   an M value, denoting some "measure"

**Text and binary encodings** A key part of the standard feature encoding is **text** and **binary** encodings. The well-known text (WKT) encoding we have shown above gives us a human-readable description of the geometry. The well-known binary (WKB) encoding is machine-readable, lossless, and faster to work with than text encoding. WKB is used for all interactions with `GDAL` and `GEOS`.

**Operations on geometries** We can break down operations on geometries for *vector* features in the following way:

-   **predicates**: a logical asserting a certain property is `TRUE`
-   **measures**: a quantity (a numeric value, possibly with measurement unit)
-   **transformations**: newly generated geometries

We can look at these operations by **what** they operate on, whether the are single geometries, pairs, or sets of geometries:

-   **unary** when it's a single geometry
-   **binary** when it's pairs of geometries
-   **n-ary** when it's sets of geometries

**Unary** predicates work to describe a property of a geometry.

A list of unary predicates:

| predicate   | meaning                                         |
|-------------|-------------------------------------------------|
| `is`        | Tests if geometry belongs to a particular class |
| `is_simple` | Tests whether geometry is simple                |
| `is_valid`  | Test whether geometry is valid                  |
| `is_empty`  | Tests if geometry is empty                      |

A list of binary predicates is:

| predicate            | meaning                                                                                                             | inverse of   |
|-----------------|-------------------------------------|-----------------|
| `contains`           | None of the points of A are outside B                                                                               | `within`     |
| `contains_properly`  | A contains B and B has no points in common with the boundary of A                                                   |              |
| `covers`             | No points of B lie in the exterior of A                                                                             | `covered_by` |
| `covered_by`         | Inverse of `covers`                                                                                                 |              |
| `crosses`            | A and B have some but not all interior points in common                                                             |              |
| `disjoint`           | A and B have no points in common                                                                                    | `intersects` |
| `equals`             | A and B are topologically equal: node order or number of nodes may differ; identical to A contains B and A within B |              |
| `equals_exact`       | A and B are geometrically equal, and have identical node order                                                      |              |
| `intersects`         | A and B are not disjoint                                                                                            | `disjoint`   |
| `is_within_distance` | A is closer to B than a given distance                                                                              |              |
| `within`             | None of the points of B are outside A                                                                               | `contains`   |
| `touches`            | A and B have at least one boundary point in common, but no interior points                                          |              |
| `overlaps`           | A and B have some points in common; the dimension of these is identical to that of A and B                          |              |
| `relate`             | Given a mask pattern, return whether A and B adhere to this pattern                                                 |              |

See the [Geometries chapter of Spatial Data Science](https://r-spatial.org/book/03-Geometries.html) for a full treatment that also covers \*\*unary and binary measures\* as well as *unary, binary and n-ary transformers*

#### Raster data model

You can read the [GDAL raster data model](https://gdal.org/user/raster_data_model.html) and [OpenGIS Grid Coverages specification](https://docs.geotools.org/stable/javadocs/org/opengis/coverage/grid/package-summary.html) but with a raster data model (a cell-based tesselation) you can read / write and operate on raster data in numerous formats using **gdal**:

```{r}
#| message: false
#| echo: false
#| error: false
library(terra)

DT::datatable(gdal(drivers = TRUE))
```

### Drainage basins and catchments - mainstems and flowpaths

When writing software for hydrologic data, concrete conceptual and logical definitions for the spatial features our software works with are critical to enabling interoperability. Along these lines, some key definitions to consider:

-   **Catchment**: A physiographic unit with zero or one inlets and one outlet. A catchment is represented by one or more partial realizations; flowpath, divide, and networks of flowpaths and divides.
-   **Nexus**: Conceptual outlet for water contained by a catchment. The hydro nexus concept represents the place where a catchment interacts with another catchment.
-   **Flowpath**: A flowpath is a linear geometry that represents the connection between a catchment's inlet and its outlet. All flowpaths have a local drainage area and may be aggregates of flowlines.
-   **Flowline**: A flowline is a linear geometry that represents a segment of a flowing body of water. Some flowlines have no local drainage area and are never aggregate features.
-   **Drainage Basin**: A catchment with zero inlets and one (internal or external) outlet.
-   **Mainstem**: The flowpath of a drainage basin.

In the following, we create a representation of catchment divides and flowpaths with the hydrologic locations that represent their nexuses.

**Note:** both the divide and flowpath are said to be realizations of the overall catchment that they represent. In other words, the divide and flowpath are both part of the representation of a catchment.

```{r}
source(system.file("extdata/new_hope_data.R", package = "nhdplusTools"))

catchment <- sf::st_geometry(new_hope_catchment)
flowpath <- sf::st_geometry(dplyr::filter(new_hope_flowline, COMID %in% new_hope_catchment$FEATUREID))
nexus <- sf::st_geometry(nhdplusTools::get_node(flowpath, "end"))


plot_window <- sf::st_as_sfc(sf::st_bbox(dplyr::filter(new_hope_catchment, FEATUREID %in% c(8891178, 8895564))))

par(mar = c(0,0,0,0))
plot(plot_window, col = NA, border = NA)
plot(catchment, col = NA, border = "grey25", add = TRUE)
plot(flowpath, col = "dodgerblue3", add = TRUE)
plot(nexus, col = "springgreen", add = TRUE, pch = 20)

```

In this example, we create a drainage basin boundary (divide), mainstem flowpath of the drainage basin, and the flowlines that make up the hydrographic network of the drainage basin.

**Fun Fact:** the word "watershed" has come to refer to the land encompassed by a drainage divide but this usage is not correct. A watershed is "a dividing ridge between drainage areas" which is in line with the non hydrologic use of the word: a crucial dividing point, line, or factor.

```{r}
basin <- sf::st_geometry(sf::st_union(new_hope_catchment, by_feature = FALSE))
flowline <- sf::st_geometry(new_hope_flowline)
mainstem <- sf::st_geometry(new_hope_flowline[new_hope_flowline$LevelPathI == min(new_hope_flowline$LevelPathI),])

par(mar = c(0,0,0,0))
plot(basin, lwd = 2) 
plot(flowline, col = "dodgerblue", add = TRUE)
plot(mainstem, col = "blue", lwd = 2, add = TRUE)
```

## Demonstration of Key Concepts in Each Language

-   drainage basins and catchments - mainstems and flowpaths
-   geospatial data representation fundamentals, simple features and geospatial grids

### Examples in R

#### sf: simple features

In R, the [`sf` package](https://cran.r-project.org/web/packages/sf/index.html) provides "*support for simple features, a standardized way to encode spatial vector data.... \[and\] Binds to 'GDAL' for reading and writing data, to 'GEOS' for geometrical operations, and to 'PROJ' for projection conversions and datum transformations.*"

When using R, you' are're using an interface to the core community standards, software, and practices (this isn't exclusive to R). TO highlight this we can install (do this once) and attach `sf` to view the external dependencies versions of the libraries linked to `sf`.

```{r}
#| message: false
# install.packages("sf")
library(sf)

sf_extSoftVersion()
```

The bindings to these lower-level C libraries, and, the larger `sf` ecosystem in R can be seen below:

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/09-sf-depends.png")
```

In the `sf` implementation in the R ecosystem stores simple feature geometries (sfg) as part of a larger data.frame using a simple feature geometry list-column (sfg). The collection of attribute and spatial information define a simple feature that can be operated on in both table (SQL) and spatial (GEOS, etc) contexts. Not only does this allow us to make the most use of the growing spatial community but *also* of the growing data science community (see `ggplot`, `dplyr`, `data.table`, `dbplyr`, `arrow`, etc.)

In practice, an `sf` object in R looks like the following:

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/sf_xfig.png")
```

We can break this down following examples presented in the recently published [Spatial Data Science](https://r-spatial.org/book/03-Geometries.html) by Edzar Pebesma and Rober Bivand. The most common simple feature geometries used to represent a *single* feature are:

| type                 | description                                                        |
|----------------------|--------------------------------------------------|
| `POINT`              | single point geometry                                              |
| `MULTIPOINT`         | set of points                                                      |
| `LINESTRING`         | single linestring (two or more points connected by straight lines) |
| `MULTILINESTRING`    | set of linestrings                                                 |
| `POLYGON`            | exterior ring with zero or more inner rings, denoting holes        |
| `MULTIPOLYGON`       | set of polygons                                                    |
| `GEOMETRYCOLLECTION` | set of the geometries above                                        |

```{r}
library(sf) |> suppressPackageStartupMessages()
par(mfrow = c(2,4))
par(mar = c(1,1,1.2,1))

# 1
p <- st_point(0:1)
plot(p, pch = 16)
title("point")
box(col = 'grey')

# 2
mp <- st_multipoint(rbind(c(1,1), c(2, 2), c(4, 1), c(2, 3), c(1,4)))
plot(mp, pch = 16)
title("multipoint")
box(col = 'grey')

# 3
ls <- st_linestring(rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)))
plot(ls, lwd = 2)
title("linestring")
box(col = 'grey')

# 4
mls <- st_multilinestring(list(
  rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)),
  rbind(c(3,0), c(4,1), c(2,1))))
plot(mls, lwd = 2)
title("multilinestring")
box(col = 'grey')

# 5 polygon
po <- st_polygon(list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),
    rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))))
plot(po, border = 'black', col = '#ff8888', lwd = 2)
title("polygon")
box(col = 'grey')

# 6 multipolygon
mpo <- st_multipolygon(list(
    list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),
        rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))),
    list(rbind(c(3,7), c(4,7), c(5,8), c(3,9), c(2,8), c(3,7)))))
plot(mpo, border = 'black', col = '#ff8888', lwd = 2)
title("multipolygon")
box(col = 'grey')

# 7 geometrycollection
gc <- st_geometrycollection(list(po, ls + c(0,5), st_point(c(2,5)), st_point(c(5,4))))
plot(gc, border = 'black', col = '#ff6666', pch = 16, lwd = 2)
title("geometrycollection")
box(col = 'grey')
```

```{r}
p
mp
ls
mls
po
mpo
gc
```

This extends the idea of ["tidy" data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) in that each row represents one observation, which has one geometric representation of the real world feature it describes.

An example of basic use:

```{r}
# Define file path
filename <- system.file("shape/nc.shp", package="sf")

# read in file
(nc <- read_sf(filename))
```

```{r}
# Map
plot(nc['SID79'])

# Spatial measures!
head(st_area(nc))

# Spatial operations!
{
  st_union(nc) |> plot()
  
  st_centroid(nc)$geometry |> plot(col = "red", add = TRUE)
}

# data science operations
library(dplyr)

{
 plot(nc$geometry)
 plot(slice_max(nc, AREA, n = 10)$geometry, 
      col = "red", add = TRUE)
 plot(slice_max(nc, AREA, n =5)$geometry, 
      col = "yellow", add = TRUE)
 plot(slice_max(nc, AREA, n = 1)$geometry, 
      col = "green", add = TRUE)
}

```

#### Measure (GEOS Measures)

Measures are the questions we ask about the dimension of a geometry, once a coordinate reference system has been established and we can compute: - How long is a line or polygon perimeter? (unit) - What is the area of a polygon? \*(unit\^2) - How far are two objects from one another? (unit)

-   *Measures* come from the GEOS library
-   *Measures* are calculated in the **units** of the projection

We'll demonstrate with a toy dataset of USGS gages in a data package from a previous workshop:

```{r}
# remotes::install_github("mhweber/awra2020spatial")
gages <- read.csv(system.file("extdata", "Gages_flowdata.csv", package = "awra2020spatial")) |> 
  dplyr::select(SOURCE_FEA, STATE, LAT_SITE, LON_SITE)
```

```{r, echo = FALSE}
dplyr::glimpse(gages)
```

Using `st_as_sf` in `sf` we can easily make a data frame with coordinates into a simple features data frame

```{r}
gages <- gages |> 
  sf::st_as_sf(coords = c("LON_SITE", "LAT_SITE"), crs = 4269)
```

```{r,  echo = FALSE}
dplyr::glimpse(gages)
```

What is the distance from the first stream gage to the second stream gage?

```{r}
sf::st_distance(gages[1,], gages[2,])
```

We can generate toy area by buffering a point (to make a polygon) and find area and perimeter. We need to project into a planar CRS to get perimeter so we will project to an Albers projection using an `epsg` code:

```{r}
poly <- sf::st_buffer(gages[1,],200)
sf::st_area(poly)
poly <- sf::st_transform(poly, 5070)
lwgeom::st_perimeter(poly)
```

#### terra for raster data

We'll look at using [terra](https://rspatial.github.io/terra/reference/terra-package.html) for working with raster data in R - [stars](https://r-spatial.github.io/stars/) is another library for working with raster data and spatiotemporal arrays (raster and vector data cubes) but for the sake of time we won't demonstrate stars in this workshop.

`terra` builds on the older `raster` package and provides methods for low-level data manipulation as well as high-level global, local, zonal, and focal computations. The predict and interpolate methods facilitate the use of regression type (interpolation, machine learning) models for spatial prediction, including with satellite remote sensing data. Processing of very large files is supported.

Like `sf`, `terra` links to GDAL, and the version and drivers can be viewed using package support functions:

```{r}
# install.packages(terra)
library(terra)

gdal()
```

```{r}
DT::datatable(gdal(drivers = TRUE))
```

We can get raster data the [National Map family of APIs](https://apps.nationalmap.gov/services/) and in particular 3DEP Elevation data and work with it using `terra` or `stars`. We'll use Mike Johnson's `AOI` package to get a county polygon to use as a template to pass to `terrainr` for our area of interest

```{r}
library(terrainr)
# remotes::install_github("mikejohnson51/AOI")
library(AOI)
library(mapview)
mapviewOptions(fgb=FALSE)

AOI::aoi_get(list("Corvallis, OR", 10, 10)) |> mapview()
```

```{r}
# aoi <- AOI::aoi_get(state = "OR", county = "Benton")
aoi <- aoi_get(list("Corvallis, OR", 10, 10))
output_tiles <- get_tiles(aoi,
                          services = c("elevation", "ortho"),
                          resolution = 30 # pixel side length in meters
                          )
```

```{r}

terra::plot(terra::rast(output_tiles[["elevation"]][[1]]))
```

```{r}
terra::plotRGB(terra::rast(output_tiles[["ortho"]][[1]]),scale = 1)
```

We can also use the \[elevatr\] package by Jeff Hollister for accessing elevation data through web services and clip out elevation to a watershed basin we access through `nhdplusTools` and the `NLDI`.

Note that `get_elev_raster` from `elevatr` will generate a set of warnings, particularly regarding retirement of `rgdal` - the package will need to be updated to conform with [retirement of rgdal, rgeos and maptools](https://r-spatial.org/r/2022/04/12/evolution.html).

```{r}
#| message: false
#| error: false
library(nhdplusTools)
library(elevatr)
library(mapview)
mapviewOptions(fgb=FALSE)
# We're passing an identifier for the stream reach (and catchment) that we know is the downstream-most segment on the Calapooia River using the COMID below
start_comid = 23763529
nldi_feature <- list(featureSource = "comid", featureID = start_comid)
basin <- nhdplusTools::get_nldi_basin(nldi_feature = nldi_feature)

x <- get_elev_raster(basin, z = 12)
# x is returned as a raster object rather than a terra spatraster, so we need to convert to spatraster
mapview::mapview(basin) + mapview(x)
```

We can also crop the elevation raster to our basin using `terra` - note that `mapview` expects `raster` objects rather than `terra` `SpatRasters` so we need to convert between `terr` and `raster`.
```{r}
library(raster)
x <- terra::mask(terra::rast(x), basin)
x <- raster::raster(x)
mapview::mapview(basin, alpha.regions = 0.02, color='blue', lwd=2) + mapview(x)
```


#### httr and jsonlite

In R, httr and jsonlite packages are used to consume the API's that provide data in json format.

**httr** httr provides us with an HTTP client to access the API with GET/POST methods, passing query parameters, and verifying the response with regard to the data format.

**jsonlite** This R package is used to convert the received json format to readable R object or data frame. `jsonlite` can also be used to convert R objects or a data frame to a json format data type.

Along with these two packages, `rlist` in R can be used to perform additional manipulation on the received json response. `list.stack` and `list.select` are two important methods exposed by `rlist` that can be used to get parsed json data into a `tibble`.

To learn about these (and other) packages, type `?httr`, `?jsonlite` and `?rlist` in the console of Rstudio to view the documentation.

Here we see an example using a JSON API from the world bank that uses a GET request below - we can see a couple ways of passing request parameters - first we'll embed the query directly in the URL for the request itself:

```{r}
#| message: false
#| warning: false
library(httr) 
library(jsonlite) 
library(dplyr)
library(data.table)

jsonResponse <- httr::GET("http://api.worldbank.org/country?per_page=10&region=OED&lendingtype=LNX&format=json")
```

We can also generate the query by passing it in a separate list we create and adding as a query parameter in the GET request

```{r}
query<-list(per_page="10",region="OED",lendingtype="LNX",format="json")
jsonResponse <- httr::GET("http://api.worldbank.org/country",query=query)
```

The `jsonlite` package can be used to parse our response which we indicated above to return as json. Why does the attempt below return an error?

```{r}
#| eval: false
#| warning: false
df <- jsonlite::fromJSON(jsonResponse)
```

HINT: what type of object is `jsonResponse` above?

```{r}
typeof(jsonResponse)
```

We can also get information about the response using `httr`:

```{r}
http_type(jsonResponse)
```

We have a list - let's take a look to figure out to pull out what we want into a data frame:

```{r}
names(jsonResponse)
```

It seems like `content` is what we want, how do we pull out?

```{r}
#| eval: false
#| warning: false
df <- jsonlite::fromJSON(jsonResponse$content)
```

Still doesn't work - we need to do some further parsing of this response using `content` in `httr` which we can then pass to `jsonlite` and the `fromJSON` function to get data into a data frame

```{r}
jsonResponseParsed <- httr::content(jsonResponse, as="parsed")
is.list(jsonResponseParsed)
names(jsonResponseParsed[[1]])
names(jsonResponseParsed[[2]])
#Hmmm
is.list(jsonResponseParsed[[2]][[1]]) 
names(jsonResponseParsed[[2]][[1]])
# now we're getting somewhere...
names(jsonResponseParsed[[2]][[2]])
```

We convert the parsed json response list to data table using `lapply` and `rbindlist`:

```{r}
df <- lapply(jsonResponseParsed[[2]],as.data.table) 
typeof(df) # still a list - one more step
dt <- rbindlist(df, fill = TRUE)
```

Some APIs like [GitHub](%22https://api.github.com%22) are easier to pull directly into a dataframe using `fromJSON`

```{r}
myGitHubRepos <- fromJSON("https://api.github.com/users/mhweber/repos")

# It returns 79 variables about my repos - we can use select to just get the ones we want to see
myGitHubRepos <- myGitHubRepos |> 
  dplyr::select(name, stargazers_count, watchers_count, language, has_issues, forks_count)
head(myGitHubRepos)
```

### Examples in Python (will be in a separate notebook in binder)

## Use Cases

### Hydro Addressing

Finding where along a river system some spatial data lies is a key use case for many modeling and analysis tasks. A full discussion is available in the [`nhdplusTools` indexing and referencing vignette.](https://doi-usgs.github.io/nhdplusTools/articles/indexing.html)

We need two inputs. Lines to index to and points that we want addresses for. With these inputs, [`nhdplusTools`](https://doi-usgs.github.io/nhdplusTools/reference/index.html#indexing-and-network-navigation) (and soon [`hydroloom`](https://doi-usgs.github.io/hydroloom/reference/index.html#indexing-and-linear-referencing)) supports generation of hydro addresses with `get_flowline_index()`.

The example below is as simple as possible. `get_flowline_index()` has a number of other capabilities, such as increased address precision and the ability to return multiple nearby addresses, that can be found in the function documentation.

```{r}
#| warning: false
source(system.file("extdata/new_hope_data.R", package = "nhdplusTools"))

fline <- sf::st_transform(sf::st_cast(new_hope_flowline, "LINESTRING"), 4326)

point <- sf::st_sfc(sf::st_point(c(-78.97, 35.916)),
                    crs = 4326)

(address <- nhdplusTools::get_flowline_index(fline, point))

plot(sf::st_geometry(fline[fline$COMID %in% address$COMID,]))+
  plot(point, add = TRUE)
```

A natural next step once we've found a hydro address is to search upstream or downstream from the location we found. [`nhdplusTools`](https://doi-usgs.github.io/nhdplusTools/reference/index.html#network-navigation) and soon [`hydroloom`](https://doi-usgs.github.io/hydroloom/reference/index.html#network-navigation-and-accumulation) offer a few upstream / downstream search functions. Here we'll show a couple from `nhdplusTools`.

```{r}
#| warning: false
up <- nhdplusTools::get_UT(fline, address$COMID)
um <- nhdplusTools::get_UM(fline, address$COMID)
dn <- nhdplusTools::get_DD(fline, address$COMID)
dm <- nhdplusTools::get_DM(fline, address$COMID)

plot(sf::st_geometry(fline), col = "grey", lwd = 0.5)
plot(sf::st_geometry(fline[fline$COMID %in% c(up, dn),]),
     add = TRUE)
plot(sf::st_geometry(fline[fline$COMID %in% c(um, dm),]),
     col = "blue", lwd = 2, add = TRUE)
plot(point, cex = 2, lwd = 2, add = TRUE)

```

The ability to navigate up or down a mainstem rather than all connected tributaries and diversions, requires some attributes that identify primary up and downstream paths. Without those paths, navigation is still possible but the "main" path navigation method is not. Below, a new function available in `hydroloom`, `navigate_network_dfs()` is shown.

```{r}
# remotes::install_github("DOI-USGS/nhdplusTools@2cb81da")
#| warning: false
net <- hydroloom::add_toids(sf::st_drop_geometry(fline), 
                            return_dendritic = FALSE)

up <- hydroloom::navigate_network_dfs(net, address$COMID, direction = "up")
dn <- hydroloom::navigate_network_dfs(net, address$COMID, direction = "dn")

plot(sf::st_geometry(fline), col = "grey", lwd = 0.5)
plot(sf::st_geometry(fline[fline$COMID %in% c(unlist(up), unlist(dn)),]),
     add = TRUE)
plot(point, cex = 2, lwd = 2, add = TRUE)
```

### Catchment Characteristics and Accumulation

[`nhdplusTools`](https://doi-usgs.github.io/nhdplusTools) (only in the latest `hydroloom` branch) and [`StreamCatTools`](https://usepa.github.io/StreamCatTools) can be used to retrieve a wide range of catchment characteristics. See the [StremCatTools Vignette](https://usepa.github.io/StreamCatTools/) for further details and examples.

```{r}
#| warning: false
library(nhdplusTools)
# remotes::install_github("USEPA/StreamCatTools")
library(StreamCatTools)
nhdplusTools::nhdplusTools_data_dir(tempdir())
cat <- sf::st_transform(new_hope_catchment, 4326)

streamcat_chars <- StreamCatTools::sc_get_params(param='name')

nawqa_chars <- nhdplusTools::get_characteristics_metadata()

StreamCatTools::sc_fullname("pctimp2016")

nawqa_chars$description[nawqa_chars$ID == "CAT_TWI"]

twi <- nhdplusTools::get_catchment_characteristics("CAT_TWI", cat$FEATUREID)

twi <- dplyr::rename(twi, CAT_TWI = "characteristic_value")

imp <- StreamCatTools::sc_get_data("pctimp2016", 'catchment', cat$FEATUREID)

cat <- dplyr::left_join(cat, imp, by = c("FEATUREID" = "COMID")) 
  # dplyr::left_join(twi, by = c("FEATUREID" = "comid"))

# plot(cat['CAT_TWI'])
plot(cat['PCTIMP2016CAT'])
```

With the two catalogs of characteristics above, we have access to nearly any characteristic imaginable and both even offer downstream accumulations of the variables. However, sometimes it is necessary to sample and accumulate data not already available. `nhdplusTools` supports accumulating drainage area and length and `hydroloom`, going forward, supports downstream accumulation more generically. Here, we'll just use one of the examples pulled above to illustrate how this works.

```{r}
library(dplyr)
accum_df <- sf::st_drop_geometry(fline) |>
  dplyr::select(COMID, FromNode, ToNode, Divergence, AreaSqKM) |>
  hydroloom::add_toids() |>
  dplyr::left_join(dplyr::select(cat, -AreaSqKM), by = c("COMID" = "FEATUREID")) |>
  sf::st_sf() |>
  dplyr::mutate(area_weighted_PCTIMP2016CAT = 
           ifelse(is.na(PCTIMP2016CAT) & AreaSqKM == 0, 
                  yes = 0, no = AreaSqKM * PCTIMP2016CAT))

accum_df <- accum_df |>
  dplyr::mutate(tot_PCTIMP2016CAT = 
           hydroloom::accumulate_downstream(accum_df, "area_weighted_PCTIMP2016CAT") / 
           hydroloom::accumulate_downstream(accum_df, "AreaSqKM"))

plot(cat['PCTIMP2016CAT'])
plot(accum_df['tot_PCTIMP2016CAT'])
```

## Data access summary -- web services and scalability

## R and Python Interoperability

We can us[Reticulate](https://rstudio.github.io/reticulate/index.html) to inter-operate easily between R and Python within RStudio and share objects between languages

```{r}
library(reticulate)
# reticulate::install_miniconda()
# create a new environment conda environment 
# conda_create("r-reticulate")

# install NumPy and SciPy
# conda_install("r-reticulate", "scipy")
# conda_install("r-reticulate", "numpy")
# conda_install("r-reticulate", "pandas")
# indicate that we want to use a specific condaenv
use_condaenv("r-reticulate")
np <- import("numpy")
print(np$version$full_version)
```

Simple examples from [reticulate - calling Python](https://rstudio.github.io/reticulate/articles/calling_python.html):

Import a module and call a function from Python:

```{r}
os <- import("os")
os$listdir(".")
```

Object conversion - When Python objects are returned to R they are converted to R objects by default - but you can deal in native Python types by choosing `convert=FALSE` in the import function

```{r}
# import numpy and specify no automatic Python to R conversion
np <- import("numpy", convert = FALSE)

# do some array manipulations with NumPy
a <- np$array(c(1:4))
print (a)

sum <- a$cumsum()

# convert to R explicitly at the end
print (py_to_r(a))

py_to_r(sum)
```

Passing Dataframes between Python and R:

```{r}
pd <- import("pandas")
penguins <- pd$read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv")
dplyr::glimpse(penguins)
```

Or like this:

```{python}
import pandas as pd
penguins =pd.read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv")
print(penguins.head())
```

Call Python from R:

```{r}
penguins <- py$penguins
dplyr::glimpse(penguins)

```

Call R from Python:

```{r}
data(iris)
iris <- iris |> 
  dplyr::filter(Petal.Length > 3)
```

```{python}
print(r.iris.head())
print(r.iris['Petal.Length'].min())
```

## Resources

### General

-   [Awesome Geospatial](https://github.com/sacridini/Awesome-Geospatial)

### R

-   [Hydroinformatics in R](https://vt-hydroinformatics.github.io/): Extensive Notes and exercises for a course on data analysis techniques in hydrology using the programming language R
-   [Spatial Data Science by Edzar Pebesma and Roger Bivand](https://r-spatial.org/book/)
-   [Geocomputation with R](https://r.geocompx.org/)
-   [r-spatial](https://github.com/r-spatial): Suite of fundamental packages for working with spatial data in R
-   [Working with Geospatial Hydrologic Data Using Web Services (R)](https://mikejohnson51.github.io/IOW2022_R/slides.html)
-   [Accessing REST API (JSON data) using httr and jsonlite](https://rstudio-pubs-static.s3.amazonaws.com/480665_ba2655419209496dbb799f1c7d050673.html)

### Python

-   [Datashader](https://datashader.org/): Accurately render even the largest data
-   [GeoPandas](https://geopandas.org/en/stable/index.html)
-   [HyRiver](https://docs.hyriver.io/index.html): a suite of Python packages that provides a unified API for retrieving geospatial/temporal data from various web services
-   [Python Foundation for Spatial Analysis](https://courses.spatialthoughts.com/python-foundation.html)
-   [Python for Geographic Data Analysis](https://pythongis.org/index.html)
-   [gdptools](https://gdptools.readthedocs.io/en/latest/) A Python package for grid- or polygon-polygon area-weighted interpolation statistics
-   [Intro to Python GIS](https://automating-gis-processes.github.io/CSC18/lessons/L2/geopandas-basics.html)
-   [xarray](https://xarray.pydata.org/en/stable/): An open-source project and Python package that makes working with labeled multi-dimensional arrays simple, efficient, and fun!
-   [rioxarray](https://corteva.github.io/rioxarray/stable/index.html): Rasterio xarray extension.
-   [GeoPandas](https://geopandas.org/en/stable/): An open-source project to make working with geospatial data in python easier.
-   [OSMnx](https://github.com/gboeing/osmnx): A Python package that lets you download and analyze geospatial data from OpenStreetMap.
-   [Xarray Spatial](https://xarray-spatial.org/master/index.html): Implements common raster analysis functions using `numba` and provides an easy-to-install, easy-to-extend codebase for raster analysis.
